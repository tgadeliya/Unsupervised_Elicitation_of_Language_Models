# Unsupervised_Elicitation_of_Language_Models

Partial implementation of the "Unsupervised Elicitation of Language Models" paper 

## How to run evaluation

1) Install project and missing dependencies
```
uv sync
```
2) Add Hypebolic API provider key in `.env` file
```
HYPERBOLIC_API_KEY=<YOUR_API_KEY>
```
3) Use file `run.py` to run evaluation. This will launch evaluation of all baselines and ICM with parameters described in the paper. Results will be printed in the CLI and plot will be saved in the root of the project (see existing plot generated by the command below)
```
uv run src/icm/run.py
```

## Implementation details

### Accuracy measurement

Models in zero-shot scenario could output neither 'true' or 'false', what requires handling this case separately. -1 value is used for this situations. Accuracy function marks as wrong label

### Zero-shot (chat)

Paper do not provide clear description of the setup. No information about whether instruction were used and it it not clear how template looks like. Brief look on both repo implementations do not clarify picture. For that reason I've decided to evalute on train set and use best performing strategy. Results presented in the paper.

{'[True\\False] template': [0.0], 'Simple instruction in the system prompt and template': [37.5],'Simple instruction in the system prompt': [43.359375]} , 'No instruction and paper template': [7.421875]}


Basedo on the results, template with `[True\False]` confusing model and model do not predict any `true` or `false` tokens. Results for prompt without any instruction and `[True\False]` template a little bit better. The best results shows simple instruction without template for the label, but important to note that result isc close to the least frequent label (113/256=0.44 for the label `True`). It is worst case even for contant label prediction.. Based on the results, option with simple instruction is the best option. Due to limited time, I haven't spend much time on instruction optimization and investigation as it is beyond the scope of the task.
Important that based on the results from the paper, they implemented zero-shot (chat) in different way, but do not provide enough description for replicate they result. 


### Golden Baseline

The whole train dataset could be easily placed into model context, so instead sampling from train dataset  I run three random seed experiments with shuffling position of the samples in the many-shot context to avoid potential recency bias




### ICM algorithm

- Paper algorithm and official implementation diverges in couple of places. Based on the task, I stick to Algorithm 1 description and tried to mention in the code all places where this happened

- Without consistency fix, alpha parameter is not useful. I omit it, because it makes diff value too low and for negative S

TODO: experiment on train dataset to verify what is better
