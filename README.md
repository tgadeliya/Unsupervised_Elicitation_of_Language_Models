# Unsupervised_Elicitation_of_Language_Models

This repository contains a partial implementation of the paper "Unsupervised Elicitation of Language Models".

## How to run evaluation

1) Install the project and dependencies
Use the following command to install the project and all required dependencies:
```
uv sync
```
2) Add the Hyperbolic API key to the .env file
Set your API key in the .env file:
```
HYPERBOLIC_API_KEY=<YOUR_API_KEY>
```
3) Run the evaluation
Use the run.py file to initiate the evaluation. This will launch the evaluation of all baselines and the ICM method using the parameters described in the paper. Results will be printed to the CLI, and a plot will be saved in the project root directory (refer to the existing plot generated by the command below):
```
uv run src/icm/run.py
```

## Implementation details

### Accuracy measurement

In the zero-shot setting, models may fail to output either 'true' or 'false', requiring explicit handling of such cases. A value of -1 is assigned to indicate these instances. The accuracy function treats such cases as incorrect predictions.


### Zero-shot (chat)

The original paper does not provide a clear description of the zero-shot setup. There is no information on whether instructions were used, nor is it clear what the prompt templates looked like. A review of the accompanying repositories did not clarify these points either. For this reason, I chose to evaluate on the training set and adopted the best-performing strategy identified through experimentation.

| Chat prompt style                                                |  Accuracy |
| ------------------------------------------------------ | --------: |
| `Only [True\False] template`                                |       0.0 |
| `Instruction & [True\False] template` |      37.5 |
| `Only instruction`              | 43.36 |
| `No instruction and paper template`                    |  7.42 |


It is important to note that the best-performing case corresponds closely to the frequency of the most common label (True) in the dataset (113 out of 256, or 44%). This suggests that the result may reflect a worst-case scenario equivalent to a constant-label baseline. Due to time constraints, no further optimization of the instructions or detailed prompt engineering was performed, as these activities are beyond the scope of this implementation.


### Golden Baseline

Because the entire training dataset can fit into the model context window, I did not sample from it. Instead, I conducted three experiments using different random seeds, shuffling the position of samples within the many-shot context to mitigate potential recency bias.

### ICM algorithm

- Paper algorithm and official implementation diverges in couple of places. Based on the task, I stick to Algorithm 1 description and tried to mention in the code all places where this happened

- Without consistency fix, alpha parameter is not useful. I omit it, because it makes diff value too low and for negative S

TODO: experiment on train dataset to verify what is better
